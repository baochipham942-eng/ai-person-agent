# AI Person Agent (AI äººç‰©åº“)

## Project Overview
A full-stack web application that aggregates, enriches, and presents comprehensive profiles of influential figures in the AI industry. Combines data from multiple sources (Wikidata, Exa.ai, GitHub, YouTube, X/Twitter) with AI-powered data cleaning to build a high-quality AI professional database.

## Tech Stack

### Frontend
- **Next.js 16.1.1** with App Router
- **React 19** + **TypeScript 5**
- **TailwindCSS 4** + **Arco Design 2.66**
- React Markdown for content rendering

### Backend
- **Next.js API Routes** (serverless)
- **NextAuth 5.0.0-beta.30** (Credentials provider)
- **Prisma ORM 5.22** with PostgreSQL
- **Neon Serverless** (WebSocket driver for fast cold starts)

### AI Integration
- **DeepSeek** - text extraction, data cleaning, JSON generation
- **Perplexity API** - complex structured data extraction
- **OpenAI** - fallback LLM
- **AI SDK 6.0.3** - unified LLM interface

### Data Sources
- Wikidata (identity, metadata)
- Exa.ai (web search, links)
- Grok/X.AI (Twitter data)
- GitHub API, YouTube Data API
- OpenAlex (academic data)

### Infrastructure
- **Vercel** - frontend/API hosting
- **Aliyun FC** - China reverse proxy
- **Inngest** - background job orchestration

## Project Structure
```
app/                    # Next.js App Router
  api/                  # API routes (admin, auth, person, search, inngest)
  (main)/               # Main layout group
  person/               # Person detail pages
lib/
  datasources/          # Multi-source integrations (wikidata, exa, grok, github, youtube)
  inngest/              # Background job workflows
  agents/               # AI agents (qa, router)
  ai/                   # DeepSeek wrapper, prompts, translators
  utils/                # Identity verification, avatar, quality scoring
  db/prisma.ts          # Singleton Prisma client (WebSocket pool)
scripts/
  enrich/               # Data enrichment pipelines (recrawl_robust, trigger_content_fetch)
  tools/                # CSV export, deduplication, cleanup
  audit/                # Data quality verification
prisma/schema.prisma    # Database models
```

## Package Manager
**Always use `bun` instead of npm/pnpm/yarn.**

## Key Commands
```bash
# Development
bun install
bun dev                     # Start dev server (port 4001)
bun dev:inngest             # Start with Inngest local dev
bun dev:all                 # Run both concurrently

# Database
bun db:generate             # Generate Prisma client
bun db:migrate              # Apply migrations
bun db:push                 # Push schema to Neon
bun db:studio               # Open Prisma Studio

# Build & Deploy
bun run build               # Production build
bun start                   # Start production (port 4001)
bun run deploy:build        # Build with asset copying for FC

# Data Scripts
bun scripts/enrich/recrawl_robust.ts         # Core person enrichment
bun scripts/enrich/trigger_content_fetch.ts  # Update GitHub/YouTube content
bun scripts/tools/export_people_csv.ts       # Export to CSV
```

## Environment Variables
```bash
DATABASE_URL=               # Neon Postgres pooler URL (with -pooler suffix)
DIRECT_URL=                 # Neon direct URL (without -pooler, for Prisma CLI)
DEEPSEEK_API_KEY=           # LLM for data cleaning
DEEPSEEK_API_URL=           # DeepSeek endpoint
EXA_API_KEY=                # Web search
GOOGLE_API_KEY=             # YouTube Data API
PERPLEXITY_API_KEY=         # Complex Q&A
INNGEST_EVENT_KEY=          # Background jobs
XAI_API_KEY=                # Grok API
XAI_BASE_URL=               # Grok endpoint
AUTH_SECRET=                # NextAuth secret
OPENAI_API_KEY=             # (Optional) fallback
```

## Key Database Models
- **People** - profiles with QID (Wikidata ID), bio, scores, status
- **PersonRole** - career history with organizations and dates
- **Organization** - companies, universities, linked to Wikidata
- **Card** - content cards (achievements, news, research)
- **RawPoolItem** - raw content from GitHub, YouTube, blogs

## Core Workflows

### Person Enrichment (`scripts/enrich/recrawl_robust.ts`)
1. Search Wikidata for identity (QID, aliases)
2. Fetch career history with dates
3. Find official links via Exa.ai
4. Extract X/Twitter via Grok
5. Merge sources, create PersonRole/Organization records

### Content Updates (`scripts/enrich/trigger_content_fetch.ts`)
- 7-day cycle per source
- GitHub: Top 10 recent repos
- YouTube: Latest 10 videos
- Blog: RSS/web via Exa

### Identity Verification (`lib/utils/identity-verifier.ts`)
- Multi-signal scoring (0-1.0)
- +0.4 QID match, +0.15 org match, +0.1 keywords
- Threshold < 0.5 = rejected

## Deployment Notes
- **Vercel**: Frontend/API hosting
- **Aliyun FC**: Use `s deploy -y --use-remote` (preserves HTTPS)
- **Neon WebSocket**: 4s â†’ 300ms cold start optimization
- **NextAuth**: Requires `trustHost: true` for proxy

## Neon Database Connection

### è¿æ¥é…ç½®
- **DATABASE_URL**: pooler è¿æ¥ï¼Œå¸¦ `-pooler` åç¼€ï¼Œåº”ç”¨è¿è¡Œæ—¶ä½¿ç”¨
- **DIRECT_URL**: ç›´è¿ï¼Œæ—  `-pooler`ï¼ŒPrisma CLI (`db push`, `migrate`) ä½¿ç”¨

### å¸¸è§é—®é¢˜

**é—®é¢˜**: `prisma db push` æŠ¥é”™ `Can't reach database server`
**åŸå› **: Neon å…è´¹ç‰ˆé—²ç½®åä¼šæš‚åœï¼Œç›´è¿å¯èƒ½è¶…æ—¶
**è§£å†³**:
1. å…ˆè¿è¡Œä»»æ„è„šæœ¬å”¤é†’æ•°æ®åº“: `npx tsx -e "import {prisma} from './lib/db/prisma'; prisma.people.count().then(console.log)"`
2. å†è¿è¡Œ `npx prisma db push`
3. å¦‚ä»å¤±è´¥ï¼Œç”¨ raw SQL æ·»åŠ å­—æ®µ: `prisma.$executeRawUnsafe('ALTER TABLE "People" ADD COLUMN IF NOT EXISTS "fieldName" TEXT')`

**é—®é¢˜**: è„šæœ¬ä¸­é€” `PrismaClientKnownRequestError` è¿æ¥æ–­å¼€
**åŸå› **: ç½‘ç»œæ³¢åŠ¨æˆ– Neon è¿æ¥æ± å›æ”¶
**è§£å†³**: è„šæœ¬å·²ä½¿ç”¨ WebSocket è¿æ¥æ± ï¼Œé€šå¸¸ä¼šè‡ªåŠ¨é‡è¿ã€‚å¦‚æŒç»­å¤±è´¥ï¼Œç­‰å¾…å‡ ç§’é‡è¯•ã€‚

### æ–°å¢äººç‰©å…¥åº“æµç¨‹
```bash
# 1. ç¼–è¾‘ scripts/enrich/add_priority_ai_people.ts æ·»åŠ äººç‰©æ•°æ®
# 2. è¿è¡Œå…¥åº“
npx tsx scripts/enrich/add_priority_ai_people.ts

# 3. æ•°æ®è¡¥å…¨
npx tsx scripts/enrich/recrawl_robust.ts           # èŒä¸šå†å²
npx tsx scripts/enrich/enrich_openalex.ts          # å­¦æœ¯å¼•ç”¨
npx tsx scripts/enrich/enrich_topics_highlights.ts # è¯é¢˜æ ‡ç­¾
npx tsx scripts/fix_missing_avatars.ts             # å¤´åƒ
```

## MCP Tools ä¼˜å…ˆçº§

å½“æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡æ—¶ï¼Œ**å¿…é¡»ä¼˜å…ˆä½¿ç”¨ MCP tools** è€Œéç¼–å†™è„šæœ¬ï¼š

| ä»»åŠ¡ç±»å‹ | MCP Tool | å›é€€æ–¹æ¡ˆ |
|---------|----------|---------|
| æ•°æ®åº“**åªè¯»**æŸ¥è¯¢ | `mcp__postgres__query` | Prisma è„šæœ¬ |
| Web æœç´¢ | `mcp__exa__web_search_exa` | lib/datasources/exa.ts |
| ä»£ç æœç´¢ | `mcp__exa__get_code_context_exa` | Grep |
| ç½‘é¡µæŠ“å– | `mcp__firecrawl__firecrawl_scrape` | WebFetch |
| ç½‘ç«™åœ°å›¾ | `mcp__firecrawl__firecrawl_map` | æ‰‹åŠ¨æ¢ç´¢ |
| GitHub PR/Issue | `mcp__github__*` | gh CLI |
| æ–‡ä»¶æ“ä½œ | `mcp__filesystem__*` | Read/Write |
| æµè§ˆå™¨è‡ªåŠ¨åŒ– | `agent-browser` (CLI) | `mcp__playwright__*` |
| æ–‡æ¡£æŸ¥è¯¢ | `mcp__context7__query-docs` | WebFetch |
| å¿«é€Ÿæœç´¢ | `mcp__duckduckgo__search` | WebSearch |

### ä½•æ—¶ä½¿ç”¨ MCP vs è„šæœ¬

**ä½¿ç”¨ MCP**ï¼š
- ä¸€æ¬¡æ€§**åªè¯»**æŸ¥è¯¢ã€å¿«é€ŸéªŒè¯
- äº¤äº’å¼æ•°æ®æ¢ç´¢ï¼ˆSELECT æŸ¥è¯¢ï¼‰
- æŸ¥çœ‹ GitHub PR/Issue
- æŠ“å–ç½‘é¡µå†…å®¹

**ä½¿ç”¨è„šæœ¬**ï¼š
- **ä»»ä½•æ•°æ®åº“å†™æ“ä½œ**ï¼ˆINSERT/UPDATE/DELETEï¼‰âš ï¸
- æ‰¹é‡æ•°æ®å¤„ç†ï¼ˆ100+ æ¡è®°å½•ï¼‰
- éœ€è¦å¤æ‚ä¸šåŠ¡é€»è¾‘
- éœ€è¦äº‹åŠ¡æˆ–é”™è¯¯é‡è¯•
- ç”Ÿäº§ç¯å¢ƒå®šæ—¶ä»»åŠ¡

> âš ï¸ **é‡è¦**: `mcp__postgres__query` æ˜¯**åªè¯»**çš„ï¼
> æ‰§è¡Œ UPDATE/INSERT/DELETE ä¼šæŠ¥é”™ï¼š`cannot execute UPDATE in a read-only transaction`
> æ‰€æœ‰å†™æ“ä½œå¿…é¡»é€šè¿‡ Prisma è„šæœ¬æ‰§è¡Œã€‚

## è„šæœ¬æ‰§è¡Œæœ€ä½³å®è·µ

### é¿å… "Prompt is too long" é”™è¯¯

é•¿æ—¶é—´è¿è¡Œçš„è„šæœ¬å¯èƒ½äº§ç”Ÿå¤§é‡è¾“å‡ºï¼Œå¯¼è‡´ Claude Code ä¸Šä¸‹æ–‡è¶…é™ã€‚è§£å†³æ–¹æ³•ï¼š

**1. ä½¿ç”¨ `--quiet` æ¨¡å¼**
```bash
npx tsx scripts/enrich/xxx.ts --quiet
```
è„šæœ¬åº”æ”¯æŒé™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºè¿›åº¦æ‘˜è¦å’Œæœ€ç»ˆç»Ÿè®¡ã€‚

**2. è¿‡æ»¤ Prisma debug æ—¥å¿—**
```bash
npx tsx scripts/xxx.ts 2>&1 | grep -v "^prisma:"
```

**3. è¾“å‡ºé‡å®šå‘åˆ°æ–‡ä»¶**
```bash
npx tsx scripts/xxx.ts > /tmp/output.log 2>&1
tail -50 /tmp/output.log  # åªæŸ¥çœ‹æœ€åéƒ¨åˆ†
```

**4. åˆ†æ‰¹å¤„ç†**
```bash
npx tsx scripts/xxx.ts --limit=50  # æ¯æ¬¡åªå¤„ç†50æ¡
```

**5. è„šæœ¬ç¼–å†™è§„èŒƒ**
```typescript
const quiet = args.includes('--quiet');
const log = (msg: string) => { if (!quiet) console.log(msg); };

// é™é»˜æ¨¡å¼ä¸‹æ¯ N æ¡è¾“å‡ºä¸€æ¬¡è¿›åº¦
if (quiet && i % 20 === 0) console.log(`è¿›åº¦: ${i}/${total}`);

// æœ€ç»ˆç»Ÿè®¡å§‹ç»ˆè¾“å‡º
console.log(`ğŸ“Š å®Œæˆ: å¤„ç† ${total} æ¡ï¼ŒæˆåŠŸ ${success} æ¡`);
```

## Browser Automation (agent-browser)

æµè§ˆå™¨è‡ªåŠ¨åŒ–**é¦–é€‰** `agent-browser` CLIï¼Œä½¿ç”¨ Ref å·¥ä½œæµï¼š

```bash
# æ ¸å¿ƒæµç¨‹
agent-browser open <url>              # æ‰“å¼€é¡µé¢
agent-browser snapshot -i             # è·å–äº¤äº’å…ƒç´  [ref=e1, e2...]
agent-browser click @e1               # ç”¨ ref ç‚¹å‡»ï¼ˆä¸ç”¨ CSS é€‰æ‹©å™¨ï¼‰
agent-browser fill @e2 "text"         # å¡«å……è¾“å…¥æ¡†
agent-browser screenshot /tmp/x.png   # æˆªå›¾
agent-browser close                   # å…³é—­

# å¸¸ç”¨å‘½ä»¤
agent-browser snapshot --json         # JSON è¾“å‡ºï¼Œä¾¿äºè§£æ
agent-browser wait 2000               # ç­‰å¾…æ¯«ç§’
agent-browser wait --load networkidle # ç­‰å¾…ç½‘ç»œç©ºé—²
agent-browser get text @e1            # è·å–å…ƒç´ æ–‡æœ¬
agent-browser --session s1 open url   # å‘½åä¼šè¯ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰
```

è¯¦ç»†æ–‡æ¡£: `.claude/skills/agent-browser/SKILL.md`

## Documentation
- `PROJECT_CONSTITUTION.md` - Architecture, deployment, error book
- `workflow_documentation.md` - Data flows, APIs, KPIs
- `README.md` - Setup guide (Chinese)
